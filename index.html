<!DOCTYPE html>
<html prefix="og: http://opg.me/ns#">
  <head>
    <meta charset="UTF-8" />
    <meta name="title" property="og:title" content="Mashr" />
    <meta name="description" property="og:description" content="Mashr is an easy-to-use data pipeline orchestration and monitoring framework for small applications">
    <meta name="type" property="og:type" content="website" />
    <!-- TODO: change website -->
    <meta name="url" property="og:url" content="https://www.mashr-framework.com" />
    <meta name="image" property="og:image" content="assets/logo/mashr_logo_mark.png" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Mat Sachs, Linus Phan, Jacob Coker-Dukowitz">
    <title>Mashr</title>
    <link rel="icon" type="image/png" sizes="32x32"
    href="assets/logo/mashr_logo_mark.png" />
    <link
    href="https://fonts.googleapis.com/css?family=Open+Sans:800,800i|Raleway:200,400,700"
    rel="stylesheet" />
    <!-- <style>reset</style> -->
    <link rel="stylesheet" href="reset.css" />
    <link rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/gruvbox-dark.min.css"
    charset="utf-8" />
    <!-- <style></style> -->
    <link rel="stylesheet" href="main.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <!-- <script></script> -->
    <script src="application.js"></script>
  </head>
  <body>
    <div class="logo-links">
        <!-- TODO: change out IDs -->
      <img src="assets/logo/mashr_logo_mark.png" alt="Mashr logo" id="mashr-logo"/>
      <a href="https://github.com/mashr-framework/mashr" target="_blank">
          <img src="assets/icons/github_white.png" alt="github logo"
          id="github-logo"/>
      </a>
    </div>
    <nav id="site-navigation">
      <ul>
        <li>
          <a href="#home" id="home-link">HOME</a>
        </li>
        <li>
          <a href="#case-study" id="case-study-link">CASE STUDY</a>
          <nav id="case-study-mobile">
            <ul></ul>
          </nav>
        </li>
        <li>
          <a href="#our-team" id="our-team-link">OUR TEAM</a>
        </li>
      </ul>
    </nav>
    <header id=home>
      <h1>
        <img src="assets/logo/mashr_logo.png" alt="Mashr logo"/>
      </h1>
      <p>a <span>data pipeline</span> framework built for the cloud</p>
    </header>
    <hr>
    <main>
      <section id="case-study">
        <h1>Case Study</h1>

        <nav>
          <ul></ul>
        </nav>

        <h2 id="introduction">1 Introduction</h2>

        <h3>1.1 What is Mashr?</h3>

        <p>Mashr is an easy-to-use data pipeline orchestration and monitoring
        framework for small applications. Mashr simplifies the process of taking
        your data from disparate sources and putting them into a single place so
        that you can use that data. It is optimized for data pipeline best
        practices including monitoring for each step of your data pipeline along
        with archiving and backup in case of failover. Mashr is built with
        Node.js, provides an easy-to-use CLI, and uses Docker to host Embulk on
        GCE Instances on Google Cloud Platform (GCP).</p>

        <p>In today’s world even small applications use a variety of tools to
        get the job done. It’s reasonable to assume that a small application
        would have data in over a dozen sources including email applications,
        ecommerce platforms, and more. Without a data pipeline that transfers
        data from multiple sources to a central repository, it can be difficult
        to perform analysis and make sound business decisions.</p>

        <p>Cloud platforms give the promise of easy-to-use solutions to answer
        problems like this. The irony, however, is that cloud platforms can
        often still be cumbersome to use and offer an overwhelming array of
        choices that can make life difficult for a developer. Mashr was built to
        simplify the process of deploying data pipelines in the cloud so that
        developers can focus on building their application and not the data
        pipeline.</p>

        <p>This case study will begin by describing some of the best practices
        for building a data pipeline, as well as the challenges that developers
        face in building their own. Next we’ll talk about some of the existing
        solutions that solve this problem, and why we built Mashr to address
        some of the drawbacks of those solutions. Finally, we’ll discuss the
        design questions we made while building Mashr and some of the unique
        challenges that came up along the way.</p>

        <h3>1.2 Why GCP?</h3>

        <p>Although we’d like for Mashr to support other cloud providers in the
        future, Mashr currently only supports setting up a data pipeline on GCP.
        GCP is by no means the only cloud provider in existence. We decided to
        start with implementing Mashr to build a data pipeline on GCP because of
        its robust set of tools for data analysis. In comparing the tools for
        big data analytics between Google’s GCP and Amazon’s AWS, GCP has an
        edge due to their varied range of services, including Tensorflow which
        is one of the most used Machine Learning libraries [3].</p>

        <h2 id="data-pipelines">2 Data Pipelines</h2>
        <h3>2.1 Use Case</h3>
        <p>In order to understand why we need data pipelines in the first place,
        let’s consider a scenario. Imagine a small web application called 
        “Bookend” that sells used books to a niche market.  Bookend has sales 
        data from their ecommerce platform, sales data from Amazon, and email 
        data from MailChimp.</p>

        <p>Bookend wants to know how many emails were opened and how “opens” 
        affected sales on their two sales platforms. This means that they need to 
        get all of these different data sources together into a single 
        destination to perform analysis on it. In addition, Bookend doesn’t just 
        want to do this once. They need to do it every day so that they can 
        understand how well their email marketing is working over time. That 
        means that they need an automated solution that can take data from these 
        different sources and combine them into a single destination.</p>

        <p>At first, it may look like their use case is simple and they could just 
        build a small application that makes API calls regularly to get data 
        into a single destination database where they can do reporting:</p>

        <img
        src="use_casea.png"
        alt="use case A"/>

        <p>Unfortunately, they need a little more than this simple diagram:</p>
        <ul>
            <li>If a job fails to load to the destination database, they want to 
            be able to keep a record of it so that they can see what went wrong 
            and so that they don’t lose any data</li>
            <li>They need to keep an archive of successful load jobs to the 
            database as well, so that they can debug issues if any data is 
            corrupted</li>
            <li>They will be performing some transformation and validation on 
            the data before it is loaded into the destination database</li>
        </ul>

        <p>Lastly, Bookend is a small but growing application and they will want 
        to add new sources of data in the future. For instance, they might begin 
        selling on another ecommerce platform like eBay, or use a different mail 
        provider or add a text messaging system. Therefore they would like an 
        easy way to add new integrations as needed without a lot of overhead.</p>

        <p>After considering those features, the diagram of what they want looks 
        more like this:</p>

        <img
        src="use_caseb.png"
        alt="use case B"/>

        <p>Data pipeline tools and services exist to address just this kind of 
        scenario. As you can see, if Bookend started their journey knowing only 
        that they needed a data pipeline, they would quickly be confronted with 
        a number of design decisions. This is why there is a large ecosystem of 
        tools and platforms that perform data pipeline services. The range of 
        these tools is vast. For instance, you have everything from Apache 
        Airflow [7] which is an open source workflow management system, to 
        Stitch [8], a proprietary solution with an easy-to-use drag-and-drop 
        UI.</p>

        <p>In order to better understand why there is such a large array of tools 
        out there and what design decisions Bookend would face, let’s consider 
        what some of the principles are of a good data pipeline for their use 
        case</p>

        <h3>2.2 Principles of a Data Pipeline</h3>
        
        <h4>2.2.1 Extract Data from the Source</h4>
        
        <p>While it may sound self-evident, we should first think about how we 
        pull data from the source. Data may come in many different formats 
        including XML, JSON, CSV, etc. Your application will need to change the 
        data into a format that the destination database can read. To do these 
        things the application will have to live somewhere, so you will have to 
        consider where you host it. You’ll also have to consider how you schedule 
        the extraction of data at a regular interval. For instance, you could 
        schedule extraction from a cron job or another tool.</p>

        <h4>2.2.2 Validate</h4>
        <p>Oftentimes we will want to perform some kind of validation on the data 
        before it reaches our destination database. Specifically, we would want 
        to ensure that the data was not corrupted in the source, corrupted by 
        the data pipeline itself, and that no data was dropped in the previous 
        steps before loading the data into the destination.</p>

        <p>For instance, imagine that one of the sources of data was a database 
        with financial transactions from the past year. We may want to make sure
         that there is a date field and that this field has valid dates within 
        the past 12 months.</p>

        <p>Validation can be an important concern because for many destination 
        databases, if you try to load data that is different than what the 
        destination expects, the load job may fail.</p>

        <h4>2.2.3 Transform</h4>
        <p>More often than not we will want to perform some sort of transformation 
        on the data [1]. This could look like a lot of different things.
        For example:</p>

        <ul>
            <li>Removing extraneous or erroneous data (cleaning), </li>
            <li>Encoding free-form values</li>
            <li>Translating coded values</li>
            <li>Deriving a new calculated value</li>
            <li>Splitting a column into multiple columns</li>
        </ul>

        <p>For example, we would want to encode free-form values if the source 
        has a field “male” that we want to map to “M” in the destination. We 
        would want to translate coded values if the source codes female as “1” 
        and the destination codes female as “F”. There are many more 
        transformations that you may need to do before data reaches the 
        destination database.</p>

        <h4>2.2.4 Staging / Archiving</h4>

        <p>As we mentioned above, the load job will fail with many databases if 
        you try to load data in the wrong format. You will need to consider what 
        to do with your data if the data fails to load to the destination database. 
        One solution is to not load data directly into the destination database. 
        Instead, the data should first enter a staging area. We then attempt to 
        perform a load job from this staging location. The data could then be 
        archived if the load job is successful, and held in the staging area if 
        the load job fails. If we keep an archive of the data, our data pipeline 
        can be more resilient to failover and we can make it easier to rollback 
        if something goes wrong. In addition to handling errors, we can use the 
        archive and staging locations to generate audit reports or diagnose and 
        repair problems.</p>

        <h4>2.2.5 Monitoring</h4>

        <p>Because load jobs are not always successful and data can become corrupted, we need an easy way to debug each step of our data pipeline. Therefore, we should have a logging service that takes account of:</p>

        <ul>
            <li>What action was taken</li>
            <li>If there were errors that occurred</li>
            <li>What data was transferred in each step of our data pipeline</li>
        </ul>

        <p>Ideally, if you were to see data corrupted in your destination 
        database then you should have a method to review every step of you data 
        pipeline to see where the corruption occurred.</p>

        <img
        src="monitoring.png"
        alt="Monitoring"/>

        <h4>2.2.6 Loading Data</h4>

        <p>Another area that appears self-evident but in fact has hidden 
        complexity is the actual method by which you load data into the 
        destination. For example, some systems like fraud detection may require 
        data to be handled in real-time or near real-time. For this reason, 
        many cloud providers offer a “streaming” method to load data into their 
        databases in addition to a “batch” loading method. There are tradeoffs 
        for either including financial cost and setup that you may need to 
        consider.</p>

        <h4>2.2.7 Orchestration and Ordering of tasks</h4>

        <p>A common source of problems in a data pipeline is that there can be a 
        large number of dependent tasks. In this example, “B” should not start 
        without “A” completing, then “C” and “D” depend on a condition in “B”.
        A data pipeline should be able to have a simple way to ensure that these 
        kinds of dependencies are easy to organize, and has mechanisms to ensure 
        the successful completion of complex tasks.</p>

        <img
        src="a_depends_on_b.png"
        alt="Orchestration and Ordering of tasks"/>

        <h4>2.2.8 Performance</h4>
        <p>Finally, a data pipeline should consider constraints on performance 
        and we should make any optimizations where possible. This is especially 
        true when dealing with a large volume or frequency of data transfer.</p>

        <p>For example, many data pipelines may make use of parallel processing. 
        A simple example of this would be taking one large file and splitting 
        it up into many smaller files that are processed at the same time.</p>

        <p>You should also examine your data pipeline for bottlenecks. These 
        frequently occur in two areas: </p>

        <ul>
            <li>Pulling data from the source: a source may be an API that 
            throttles too many requests or a database with read limits. </li>
            <li>Loading data to the destination database: usually because of 
            write limits or other constraints specific to the kind of database 
            that you are loading data into.</li>
        </ul>

        <h3>2.3 Flow of Data</h3>
        <p>Once you’ve thought through which of these features your data 
        pipeline will implement and how it will implement them, you’ll then want 
        to consider in what order your data pipeline flows. There are two 
        specific subsets of data pipelines that you may hear about.</p>

        <h4>2.3.1 ETL</h4>
        <p>The first subset is ETL, which stands for “Extract, Transform, Load”. 
        This follows the traditional flow of data for most data pipelines. Due 
        to the cost of storing data, you would want to perform any 
        transformations that reduce the amount of data before loading it to the 
        destination database [6]. For instance you may want to perform 
        aggregations on the data or another step that would reduce the amount 
        of data before loading.</p>


        <img
        src="etl,png"
        alt="Extract, Transform and Load"/>

        <h4>2.3.1 ELT</h4>
        <p>The other subset is ELT, which stands for “Extract, Load, Transform”. 
        With the advent of cloud platforms like Google Cloud Platform and AWS, 
        the cost of storing data is much cheaper and therefore is no longer as 
        great a concern [6]. Therefore, transformations like aggregation can 
        occur after we store data.</p>

        <img
        src="elt.png"
        alt="Extract, Load and Transform"/>

        <h3>2.4 Components of a data pipeline</h3>
        <p>After reviewing the principles of a good data pipeline for our use 
        case, we can see that there are four major components of a data pipeline 
        from an infrastructure perspective:</p>

        <img
        src="data_pipeline_components.png"
        alt="Components of a Data Pipeline"/>

        <h3>2.5 Challenges of Building a data pipeline</h3>

        <p>We have already shown that there are actually a lot of principles and 
        decision points to consider when building a data pipeline solution that 
        automatically transfers data from one place to another. However, to gain 
        an even deeper understanding of the challenges of building a data 
        pipeline, let’s briefly explore what it takes to set up a data pipeline 
        on Google Cloud Platform (GCP).</p>

        <h4>2.5.1 Overwhelming number of configuration options</h4>
        <p>Setting up a data pipeline on GCP can be a complicated and 
        challenging task because of the large number of services that are 
        offered that may be relevant for setting up a data pipeline. For 
        example, some of the services that are currently offered are GCP’s Data 
        Transfer Service, DataFlow, Cloud Functions, Cloud Run, and so much 
        more. Additionally, new products come out almost every day, and existing 
        products are changed frequently. It starts to get overwhelming quick.</p>

        <p>After deciding which services to use, the developer still has a lot 
        of work to do before being competent in using those resources. As an 
        example, this table shows a count of how many API calls and GCP services 
        that Mashr manages in GCP to set up a data pipeline. A developer setting 
        out to create their own data pipeline would need to be familiar with 
        each of these services and the configuration options available to each 
        service. This table also doesn’t mention the countless validations as 
        well as the setup required for logging and other helpful tools that 
        Mashr provides.</p>

        <img
        src="mashr_api_calls_to_gcp_table.png"
        alt="Number of API calls to GCP: 23; Number of GCP Services used: 7"/>

        <h4>2.5.3 Incomplete libraries</h4>
        <p>Also, some of the client libraries for communicating with GCP 
        resources may not be complete. For instance, there is no node.js client 
        library to manage cloud functions at the time of this writing [11].</p>

        <h4>2.5.4 Individual “tasks” are comprised of many smaller tasks</h4>
        <p>Finally, there is rarely a single task that doesn’t involve a series 
        of smaller tasks. For instance, creating a virtual machine instance on 
        the cloud with GCP requires you to also determine what OS you need and 
        how much memory it should have [10]. As another example, setting up a 
        Google Cloud Storage Bucket requires you to determine which Storage 
        Class to use [9].</p>

        <h2 id="frameworks">3 Solutions</h2>

        <p>Due to the complexity of setting up a data pipeline on your own,
        there are many solutions currently in existence that seek to ease the
        process.</p>

        <h3>3.1 Hosted Solutions</h3>

        <p>First, there are “hosted” solutions, which are either fully or
        partially proprietary solutions that abstract away server setup and
        deploys an entire data pipeline for you.</p>

        <p>Most of these services provide a graphical interface and
        plug-and-play functionality. They range from enterprise grade services
        like Informatica PowerCenter that can cost 6 figures, to tools such as
        Stitch and FiveTran which provide affordable tiers to small and medium
        sized companies.</p>

        <p>There are several good reasons to go with a hosted solution:</p>
        <ul>
          <li>Both the infrastructure and the processes for dealing with
          extraction and loading is abstracted from the user—there are no
          servers to consider.</li>
          <li>Many deal with failover and backup of your data.</li>
          <li>Also, many have a graphical user interface which makes these
          solutions convenient and easy to use.</li>
        </ul>

        <p>There are also some downsides:</p>
        <ul>
          <li>They cost money.</li>
          <li>Data sits on someone else’s server which is just one more
          vulnerability you have to worry about.</li>
          <li>There is less control and customization available to you as a
          developer.</li>
        </ul>

        <img src="#"
        alt="pros and cons table for hosted solutions"/>

        <h3>3.2 Self-Hosted Solutions</h3>

        <p>The second group of solutions are "self-hosted" solutions. These are
        open source tools which allow you to implement your own data pipeline.
        These solutions don’t handle hosting for you and require more
        configuration. Unfortunately, many of these solutions are
        unmaintained.</p>

        <p>As an example, one of the surviving and most successful of these open
        source platforms is Pentaho Data Integration (PDI) [12] which integrates
        with over 300 services. PDI's graphical interface can be used to develop
        a simple ETL data pipeline to have steps for extraction, transformation,
        and loading.</p>


        <p>The pros of self-hosted solutions are:</p>
        <ul>
          <li>They are very customizable. You can edit or extend the code as
          needed. If an integration doesn’t exist that you need you could always
          build one.</li>
          <li>You don’t need to worry about the data sitting on anyone else’s
          servers.</li>
          <li>They are cheap.</li>
        </ul>

        <p>The downside to using a self-hosted solution is that their
        customizability comes at a cost—both financially and in terms of
        time:</p>
        <ul>
          <li>If you want a fully automated solution you will still need to host
          it somewhere.</li>
          <li>You will need to learn the application and set up the
          infrastructure for it. For example, you will likely need to worry
          about managing the scheduling, archiving, monitoring, or failover for
          your data.</li>
        </ul>

        <img src="#"
        alt="pros and cons table for self-hosted solutions"/>

        <h3>3.3 Mashr</h3>

        <p>Recall that Mashr is built for small applications that have data
        spread out over a variety of data sources such as a psql database,
        salesforce, or other applications with REST API’s. Developers for the
        small application want to combine all of that data into one place so
        they can use it.</p>

        <p>Since the users in our scenario are developers, they would also like
        the ability to customize the code if needed. For example, they may want
        to add their own transformation or validation steps. Additionally, if
        the solution doesn’t come with an integration for a particular source,
        they’d like the ability to create it.</p>

        <p>Using a self-hosted solution sounds better since it is customizable,
        and already partially implemented. However these solutions usually don’t
        account for the hosting, scheduling, monitoring, or failover necessary
        in a production environment. They still take a lot of time to set up
        hosting and to make all of the orchestration decisions.</p>

        <p>This is where Mashr fits in—Mashr’s goal is to provide all the
        extraction, transformation, and loading functionality a small
        application needs, while being customizable and simple for a developer
        to deploy. Mashr does all of this while also taking into account the
        best practices for setting up a data pipeline.</p>

        <img src="#"
        alt="showing where mashr sits on scale of level of abstraction"/>

        <p>By “Abstracted” in the image above we mean how much of the hosting,
        management and configuration a user has to worry about.</p>

        <p>Mashr sits somewhere in the middle. It is less abstracted than a
        hosted solution, and more abstracted than a self-hosted solution. Mashr
        makes decisions for the user about the scheduling and running of jobs,
        how to recover data that fails to load, or where to archive data for
        future reference.</p>

        <img src="#"
        alt="showing where mashr sits on scale of level of customizability"/>

        <p>Similarly with “customizability”, Mashr is positioned between hosted
        and self-hosted solutions. Since Mashr is open source, a user can alter
        the code when necessary.</p>

        <p>Hosted solutions are closed systems that have limited
        customizability. At the same time, since Mashr has more functionality
        built-in than many self-hosted options, it’s less customizable compared
        to typical self-hosted solutions. Those solutions leave hosting and
        orchestration details up to the user to customize themselves.</p>

        <p>The pros for using Mashr include:</p>
        <ul>
          <li>You don’t have to worry about the data sitting on someone else’s
          servers.</li>
          <li>More abstraction than a self-hosted solution; it manages the set
          up of infrastructure for your data pipeline.</li>
          <li>More customizability than the hosted solutions. You can add new
          integrations, and it provides an easy way to add your transformation
          and validation steps.</li>
        </ul>

        <p>The cons for using Mashr are that:</p>
        <ul>
          <li>While Mashr sets up the data pipeline infrastructure, it leaves it
          up to the user to manage it. For instance, the user may have to go
          into those resources and debug them if an error appears.</li>
          <li>Mashr is opinionated about how a data pipeline should be set up on
          GCP. If your use case is more complicated or you want to use other GCP
          tools then that would mean editing the actual code base of Mashr or
          using GCP’s UI.</li>
        </ul>

        <img src="#"
        alt="pros and cons table for mashr"/>

        <h2 id="mashr-design">4 Mashr Design</h2>

        <h2 id="challenges">5 Challenges</h2>
        <p>The process of designing and building Mashr came with many challenges. 
        We’ll review some of the most interesting challenges below.</p>

        <h3>5.1 Docker Containers</h3>

        <p>The first challenge was that Mashr launches Docker containers in a 
        non-standard way. We knew we wanted to host Embulk on Docker in our GCE 
        instance. However, each container for each integration would have a 
        different state because the files and command that Embulk would use to 
        run a particular job or integration would be different based on what 
        input the user supplied in the mashr configuration file. For example, 
        the dependencies that Embulk needs to run a particular input job are 
        dependent on what type of input the user wants (e.g., REST api or psql 
        database). Therefore that we can’t just host an image on Docker Hub 
        that we pull every time we create a GCE instance for an integration. 
        Instead we have to build the image on the fly inside the GCE instance. 
        This meant that we have to dynamically generate the files that the 
        container uses, copy them to the GCE instance, and then copy those 
        files into the Docker container as its being built.</p>

        <p>We do that in a couple of steps:</p>
        <ul>
            <li>We convert template files to strings.</li>
            <li>
                We interpolate the values that the user supplied in the 
                <code>mashr_config.yml</code> file into those template file 
                strings.
            </li>
            <li>
                We create files from the strings and copy those files to the 
                GCE instance using a startup script. One of the files copied 
                over is the Dockerfile.
            </li>
            <li>We then build the Docker image inside the startup script.</li>
            <li>
                When the Docker image is built, it includes commands to copy 
                files from the GCE instance into the container.
            </li>
            <li>
                Then we run the container with all of the files it needs in it.
            </li>
        </ul>

        <img
        src="docker_container_creation_process.png"
        alt="Process of creating the docker container"/>

        <h3>5.2 Docker Cron Jobs</h3>
        <p>Another issue that we ran into was that Embulk doesn’t have its own 
        built-in clock to schedule jobs, so we needed to find a way to schedule 
        the Embulk jobs to run at a regular interval.</p>

        <p>Our options to do this were either:</p>
        <ul>
            <li>
                A cron job on the virtual machine that the container runs on
            </li>
            <li>Cloud Scheduler, GCP’s built in cron service</li>
            <li>A cron job that runs in a separate container</li>
            <li>A cron job running inside the Docker container itself</li>
        </ul>

        <p>Because running a cron job on the virtual machine is one more 
        dependency on the type of OS that we don’t want to include, we can 
        quickly discount that option.</p>

        <p>Running a cron job in a separate container made some sense to us 
        because it would be following the principle that each specific service 
        has its own Docker container. Also, using Cloud Scheduler is appealing 
        for the same reason—it allows us to separate concerns and have a 
        dedicated fully managed service for a particular task.</p>

        <p>However, we felt like both of those options added additional 
        complexity that didn’t make sense for our use case. Therefore we decided 
        that in this particular instance, it was reasonable for our Docker 
        container to both manage the running of Embulk jobs and the scheduling 
        of those jobs. Therefore we went with option (d), a cron job running 
        inside the same docker container that runs the Embulk job.</p>

        <p>However, choosing to run the cron job inside the Docker container 
        raised an additional concern. To have a cron job running on a Docker 
        container requires that the container is always active and never shuts 
        down. A Docker container runs as long as a process is running inside 
        of it.</p>

        <p>In order to accomplish this, the last command in our Dockerfile is a 
        <code>tail -f</code> command which watches a particular file 
        indefinitely. This keeps the container running in the background. 
        Additionally, our <code>run</code> command used to start the container 
        has a “restart” policy of true. This ensures that if for any reason the 
        container is stopped that it will restart on its own.</p>

        <img
        src="docker_tail_f_command.png"
        alt="CMD service cron start && tail -f /var/cron/cron.lg"/>

        <h3>5.3 Docker Logging</h3>
        <p>It was important to us that users had an easy way to see the logs 
        for every step of the process. This included what events occurred, 
        errors if there were any, and data that was transferred between steps. 
        Since Mashr uses GCP it makes sense that we use GCP’s logging service, 
        "Stackdriver". Stackdriver is a great logging tool that aggregates each 
        GCP project’s logs, metrics and events into a single filterable 
        dashboard. Therefore for each step of our data pipeline—the Embulk GCE 
        Instance, the buckets and the function—we had to ensure that logs were 
        properly sent to Stackdriver.</p>

        <p>Prior to the configuration of sending the Docker logs to Stackdriver, 
        if there were any errors on an Embulk cron job the user wouldn’t have a 
        way of knowing that an error occurred without actually going into the 
        Docker container. To do this they would have to SSH into the GCE 
        Instance, <code>exec</code> into the Docker container, find where the 
        logs are stored, and then output them to stdout for viewing.</p>

        <p>Therefore we had to find a way of getting the <code>stderr</code> 
        and <code>stdout</code> of our Embulk cron jobs running inside a docker 
        container in the GCE Instance, into Stackdriver.</p>

        <p>We used Docker’s logging driver for GCP to accomplish this. There is 
        an option to use the GCP logging driver in the run command or in a 
        configuration file for Docker called <code>daemon.json</code>. Docker 
        will then detect your credentials from inside the GCE Instance and will 
        send logs to Stackdriver for you.</p>

        <p>However, normally cron logs would not be captured by Docker’s logging 
        service by default. You have to send the logs to a specific process that 
        Docker listens to. Ultimately we were able to accomplish this by running 
        the command for the cron job with <code>>> /proc/1/fd/1 2>&1</code>. 
        This appends the <code>stdout</code> and <code>stderr</code> of the 
        command preceding it to the process that Docker is watching.

        <img
        src="docker_logging_script.png"
        alt="Docker Logging Script"/>

        <h2>6 Future Work</h2>
        <p>A software engineering project is never actually complete, and the 
        same can be said for Mashr. Below are ideas to further improve the 
        Mashr project.</p>

        <p>Some of the things that we’d like to include in the future are:</p>
        <ul>
            <li>Enable cross platform support - AWS!</li>
            <li>Enable other target destinations</li>
            <li>Enable users authentication with OAuth instead of keyfiles.</li>
            <li>
                Automatic schema pre-check and creation to ensure that your input values will upload to BigQuery successfully.
            </li>
            <li>
                Add a ‘redeploy’ command that allows users to overwrite existing 
                integrations.
            </li>
        </ul>

        <section id="footnotes">
          <h2 id="references">7 References</h2>

          <h5>7.1 Footnotes</h5>
          <ol>
            <li id="footnote-1"><a
                href="https://en.wikipedia.org/wiki/Extract,_transform,_load"
                target="_blank">https://en.wikipedia.org/wiki/Extract,_transform,_load</a></li>
            <li id="footnote-2"><a href="https://www.sdxcentral.com/articles/news/aws-remains-dominant-player-in-growing-cloud-market-srg-reports/2019/02/" target="_blank">https://www.sdxcentral.com/articles/news/aws-remains-dominant-player-in-growing-cloud-market-srg-reports/2019/02/</a></li>
            <li id="footnote-3"><a
                href="https://www.edureka.co/blog/google-cloud-vs-aws/#BDAC"
                target="_blank">https://www.edureka.co/blog/google-cloud-vs-aws/#BDAC</a></li>
            <li id="footnote-4"><a href="https://blog.panoply.io/a-full-comparison-of-redshift-and-bigquery" target="_blank">https://blog.panoply.io/a-full-comparison-of-redshift-and-bigquery</a></li>
            <li id="footnote-5"><a href="https://www.singer.io/" target="_blank">https://www.singer.io/</a></li>
            <li id="footnote-6"><a href="https://www.youtube.com/watch?v=wiwabNQzRJc" target="_blank">https://www.youtube.com/watch?v=wiwabNQzRJc</a></li>
            <li id="footnote-7"><a href="https://airflow.apache.org/" target="_blank">https://airflow.apache.org/</a></li>
            <li id="footnote-8"><a href="https://www.stitchdata.com/" target="_blank">https://www.stitchdata.com/</a></li>
            <li id="footnote-9"><a href="https://cloud.google.com/storage/docs/storage-classes" target="_blank">https://cloud.google.com/storage/docs/storage-classes</a></li>
            <li id="footnote-10"><a href="https://cloud.google.com/compute/docs/machine-types" target="_blank">https://cloud.google.com/compute/docs/machine-types</a></li>
            <li id="footnote-11"><a href="https://cloud.google.com/nodejs/docs/" target="_blank">https://cloud.google.com/nodejs/docs/</a></li>
            <li id="footnote-12"><a href="https://help.pentaho.com/Documentation/7.1/0D0/Pentaho_Data_Integration" target="_blank">https://help.pentaho.com/Documentation/7.1/0D0/Pentaho_Data_Integration</a></li>
          </ol>
<!--
          <h5>7.2 Resources</h5>
          <ol>
            <li><a
                href="https://www.manning.com/books/serverless-architectures-on-aws"
                target="_blank">Serverless Architecture on AWS</a></li>
            <li><a href="https://www.manning.com/books/aws-lambda-in-action" target="_blank">Lambda in Action</a></li>
            <li><a href="https://www.youtube.com/playlist?list=PLEx5khR4g7PJNproQQ4SZ96Qeu-kr-Xbn" target="_blank">GOTO Conferences 2018</a></li>
          </ol>

          <h5>7.3 AWS Documentation</h5>
          <ul>
            <li><a
                href="https://docs.aws.amazon.com/lambda/latest/dg/welcome.html"
                target="_blank">What is AWS Lambda?</a></li>
            <li><a
                href="https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html"
                target="_blank">What is Amazon API Gateway?</a></li>
            <li><a
                href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html"
                target="_blank">What is Amazon DynamoDB?</a></li>
            <li><a
                href="https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Lambda.html"
                target="_blank">AWS Lambda SDK</a></li>
            <li><a
                href="https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/APIGateway.html"
                target="_blank">AWS API Gateway SDK</a></li>
            <li><a
                href="https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB/DocumentClient.html"
                target="_blank">AWS DynamoDB Document Client SDK</a></li>
          </ul>
-->
        </section>
      </section>
    </main>
    <section id="our-team">
      <h1>Our Team</h1>
      <p>We are looking for opportunities.  If you liked what you saw and want to talk more, please
      reach out!</p>
      <ul>
        <li class="individual">
          <img src="https://s3.amazonaws.com/bam-lambda/images/jason.png" alt="Jason Overby"/>
          <h3>Jason Overby</h3>
          <p>Portland, OR</p>
          <ul class="social-icons">
            <li>
              <a href="mailto:j@jasonoverby.com"
                 target="_blank"><img src="https://s3.amazonaws.com/bam-lambda/images/email.png"
                 alt="email"/></a>
            </li>
            <li>
              <a href="http://www.jasonoverby.com"
                 target="_blank"><img src="https://s3.amazonaws.com/bam-lambda/images/www.png"
                 alt="website"/></a>
            </li>
            <li>
              <a href="https://www.linkedin.com/in/jasoncoverby/"
                 target="_blank"><img src="https://s3.amazonaws.com/bam-lambda/images/linkedin.png"
                 alt="linkedin"/></a>
            </li>
          </ul>
        </li>
        <li class="individual">
          <img src="https://s3.amazonaws.com/bam-lambda/images/jocie.png" alt="Jocie Moore"/>
          <h3>Jocie Moore</h3>
          <p>San Francisco, CA</p>
          <ul class="social-icons">
            <li>
              <a href="mailto:hello@jociemoore.com"
                 target="_blank"><img src="https://s3.amazonaws.com/bam-lambda/images/email.png"
                 alt="email"/></a>
            </li>
            <li>
              <a href="https://www.jociemoore.com"
                 target="_blank"><img src="https://s3.amazonaws.com/bam-lambda/images/www.png"
                 alt="website"/></a>
            </li>
            <li>
              <a href="https://www.linkedin.com/in/jocie-moore/"
                 target="_blank"><img src="https://s3.amazonaws.com/bam-lambda/images/linkedin.png"
                 alt="linkedin"/></a>
            </li>
          </ul>
        </li>
        <li class="individual">
          <img src="https://s3.amazonaws.com/bam-lambda/images/tak.png" alt="Takayoshi Sampson"/>
          <h3>Tak Sampson</h3>

          <p>New York, NY</p>
          <ul class="social-icons">
            <li>
              <a href="mailto:tak.sampson@gmail.com"
                 target="_blank"><img src="https://s3.amazonaws.com/bam-lambda/images/email.png"
                 alt="email"/></a>
            </li>
            <li>
              <a href=""
                 target="_blank"><img src="https://s3.amazonaws.com/bam-lambda/images/www.png"
                 alt="website"/></a>
            </li>
            <li>
              <a href="https://www.linkedin.com/in/takayoshi-sampson-780b66168/"
                 target="_blank"><img src="https://s3.amazonaws.com/bam-lambda/images/linkedin.png"
                 alt="linkedin"/></a>
            </li>
          </ul>
        </li>
      </ul>
      <small>This website was <a
          href="https://medium.com/bam-lambda/building-bam-lambda-website-1a28a3b53f6a"
          target="_blank">made with BAM!</a></small> </section>
  </body>
</html>
