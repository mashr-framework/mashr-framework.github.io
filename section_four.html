        <h2 id="mashr">Mashr</h2>

        <p>Mashr was made to be an easy-to-use framework with just a few
        commands so that developers can get started quickly building their own
        data pipelines:</p>

        <ul>
          <li>
            <code class="command">init</code>
            <span>
              - creates a YAML configuration file in the users working
              directory
            </span>
          </li>
          <li>
            <code class="command">deploy</code>
            <span>
              - launches all of the GCP resources to create the data pipeline
            </span>
          </li>
          <li>
            <code class="command">destroy</code>
            <span>
              - destroys all of the GCP resources of a specific data pipeline
            </span>
          </li>

          <li>
            <code class="command">list</code>
            <span>
              - lists your current data pipelines
            </span>
          </li>
          <li>
            <code class="command">help</code>
            <span>
             - help text for Mashr
            </span>
          </li>
        </ul>

        <p>
          The workflow for a developer looks like this:
          <ul>
            <li>
              First, you would run <code>mashr init</code> which sets up the user’s
              current working directory as a mashr directory. It would create a
              <code>mashr_config.yaml</code> file in the user’s current working
              directory.
            </li>
            <li>The user then fills out the <code>mashr_config.yml</code> file
              to tell Mashr what data source it will be pulling from and what
              BigQuery dataset and table the data should go to.
            </li>
            <li>
              Then you run <code>mashr deploy</code>, this launches all of the
              resources
              needed.
            </li>
            <li>
              Finally, <code>mashr destroy</code> will take down all those
              resources.
            </li>
          </ul>
        </p>

        <p>
          Before going into detail about the data pipeline that Mashr deploys
          and what these commands do behind the scenes, it’s important to
          understand the tools and GCP services that Mashr uses.
        </p>

        <h3>GCP services used</h3>
        <p>
          <strong>INSERT GCP TABLE HERE</strong>
        </p>
        <p>
          When Mashr deploys to GCP it makes use of a number of services,
          shown in the table above.  Readers may already be familiar with AWS’s
          cloud services, so we’ll name the equivalent service on AWS to help
          build a frame of reference.
        </p>

        <h3>4.1.1 Google Compute Engine (GCE)</h3>
        <p>
          GCE is the Infrastructure-as-a-Service (IaaS) component of GCP. A GCE
          Instance is a virtual machine. If you’re familiar with AWS this is
          similar to an EC2 instance.
        </p>

        <h3>4.1.2 Google Cloud Storage (GCS). </h3>
        <p>
          GCS is a data storage service similar to AWS’s S3. GCS users create
          “buckets” to store data. Each bucket can have different settings that
          affect the speed and price of the storage. For example Mashr uses
          both “Multi-Regional” and “Coldline” storage. Multi-Regional is
          faster but more expensive than Coldline storage.
        </p>

        <h3>
          4.1.3 Cloud Function
        </h3>
        <p>
          Cloud Functions are GCP’s implementation of a Function-as-a-Service
          (FaaS) platform, similar to AWS Lambda. It executes custom code on
          fully-managed servers in response to event triggers. Since the
          servers are fully managed, this removes the work needed to manage the
          infrastructure that the code runs on.
        </p>

        <h3>
          4.1.4 BigQuery
        </h3>
        <p>
          BigQuery is GCP’s data warehouse, similar to AWS’s Redshift. BigQuery
          enables interactive analysis of massively large datasets. It is an
          analytical engine that provides online analytical processing (OLAP)
          performance.
        </p>

        <h3>
          4.1.5 Stackdriver
        </h3>
        <p>
          Stackdriver is GCP’s logging service, similar to AWS’s CloudTrail or
          CloudWatch. Stackdriver aggregates metrics, logs, and events from
          cloud and on-premises infrastructure. Developers can use this data to
          quickly pinpoint causes of issues across a system of services on GCP.
        </p>

        <h2>Embulk</h2>
        <p>
          Embulk is a core component of Mashr so we’ll also need to understand
          what Embulk is and how we use it.
        </p>

        <h3>4.2.1 What is Embulk</h3>
        <p>
          Embulk is an open source plugin-based data loader. By “plugin-based”,
          it means that a user can choose from an existing set of plugins for
          their input and output data sources. They can also build their own
          plugins if there is not an existing one for their data source.
        </p>

        <p>
          You download Embulk as a JRuby application. Then you can download
          various gems (dependencies) which are the input and output plugins
          that allow you to transfer data from one service to another. Embulk
          allows input and output plugins to communicate with each other via
          Ruby objects in a standard format so that any input data source can
          talk to any output destination. You can read more about Embulk <a
            href="https://www.embulk.org/docs/">here.</a>
        </p>

        <h3>
          4.2.2 How Embulk works
        </h3>
        <p>
          After downloading the input and output plugins, a user specifies what
          input and output you want for a specific Embulk job with a YAML file.
        </p>
        <p>
        [ INSERT IMAGE HERE ]
        </p>
        <p>
          After completing the yaml file, an Embulk job is run in the terminal.
          There are different commands to run an Embulk job depending on your
          use case. This is important, and later we will show you how users can
          specify the Embulk run command for their data pipeline in Mashr.
        </p>

        <p>
          Two commands that are important to know are:
          <ul>
            <li>
              <code>embulk run config.yaml</code>
              <span>
                - Runs an embulk job
              </span>
            </li>
            <li>
              <code>embulk run config.yml -c diff.yml</code>
              <span>
                - Runs an embulk job with incremental loading
              </span>
            </li>
          </ul>
        </p>

        <p>
          An “incremental load job” in Embulk is a job that tracks what data
          has been loaded to prevent sending values that have already been
          loaded to the destination. To do this, Embulk will create a second
          YAML file that keeps track of the last values loaded.
        </p>

        <p>
          In this example embulk yaml file, you can see that the value for
          <code>incremental</code> is set to <code>true</code>, and that the
          columns that Embulk will track for incremental loading are
          <code>created_at</code> and <code>id</code>. When we run this job
          with the command <code>embulk run config.yml -c diff.yml</code>,
          Embulk will create a separate <code>diff.yml</code> file that keeps
          track of the last values of <code>created_at</code> and
          <code>id</code> that have been loaded. Embulk will then check those
          values the next time <code>embulk run config.yml -c diff.yml</code>
          runs.
        </p>

        <h3>
          4.2.2 How Mashr uses Embulk
        </h3>
        <p>
          Recall that earlier we talked about how data pipelines are composed
          of several components. Mashr uses Embulk as the data extraction
          component of the data pipeline it creates on GCP.
        </p>
        <p>
          Mashr requires that a user completes a <code>mashr_config.yml</code>
          file which specifies several values that Mashr needs to build the
          data pipeline on GCP, as well as the input source that the user wants
          to pull data from. Mashr parses this YAML file to generate various
          resources including a new YAML file which our Embulk application
          hosted on a GCE Instance will use later on. We’ll discuss more about
          how this works on section 4.4.1 when we talk about the command
          <code>mashr init</code>.
        </p>

        <h3>
          4.2.3 Why we chose Embulk
        </h3>
        <p>
          When building Mashr, we first considered how users would supply data
          to the pipeline without having to write code. We wanted something
          modular and easy to use. There were several options we looked at. We
          decided on Embulk because it has a large open source community and
          well maintained code base.
        </p>

        <h2>
          4.3 Mashr Architecture
        </h2>
        <p>
        [ INSERT IMAGE HERE ]
        </p>
        <p>
          Now that we have a basic understanding of the GCP services and tools
          that Mashr uses, let’s talk about what Mashr does and how it was
          built. This diagram shows a high-level overview of Mashr’s main
          components. When you run <code>mashr deploy</code> in your terminal,
          the following actions take place:
        </p>
        <p>
          Mashr sets up a GCE instance, with Embulk running on a Docker
          container. The container has a cron job running that pulls data from
          the source and loads it into Google Cloud Storage. Adding the data to
          GCS triggers the Cloud Function. The Cloud Function attempts to load
          the data into BigQuery. If the load is successful, the Cloud Function
          moves the data file from the staging bucket to the archive bucket. If
          the load is not successful, the data file remains in the staging
          bucket for debugging purposes. Each step of this process has
          Stackdriver monitoring enabled so users can debug the data pipeline
          if necessary.
        </p>
        <p>
          Let’s dive into the components of Mashr so we can break down each
          step described above. We’ll start by talking about Embulk and the GCE
          instance it is hosted on.
        </p>

        <h3>
          4.3.1 Google Compute Engine & Docker
        </h3>
        <p>
        [ INSERT IMAGE HERE ]
        </p>
        <p>
          The data extraction component needs to be hosted somewhere. We
          considered using a Cloud Function, but this wouldn’t work for two
          reasons. First, Embulk is written in a runtime that is unsupported by
          Cloud Functions. Second, we needed to persist data between runs to
          track the most recent record extracted, however Cloud Functions are
          ephemeral and therefore do not persist data between runs on their
          own.
        </p>
        <p>
          We decided it made sense to use a GCE instance to host Embulk, which
          allows us maximal flexibility and the ability to persist data between
          jobs.
        </p>
        <p>
          Hosting on a GCE Instance raised the concern of OS dependency. For
          instance, GCP recently discontinued support for GCE instances running
          Debian 8. Over time, as VMs evolve and older versions are dropped
          from the GCE library, Mashr may break and need to be reconfigured to
          run on new OSs. It’s fine to upgrade over time, but we want to do
          upgrades on our terms rather than be forced to update on someone
          else’s schedule. So we put Embulk in a Docker container to make the
          application resilient to technology changes. We’ll discuss this
          process in more detail in section 5.
        </p>
        <p>
        [ INSERT IMAGE HERE ]
        </p>
        <p>
          We built a custom Docker image that creates a container with several
          features and customizations depending on each particular
          integration’s configuration. For instance, each container will need
          different Embulk plugins installed and different Embulk run commands
          depending on the user’s needs.
        </p>
        <p>
          Additionally, we implemented the scheduling of Embulk jobs with a
          cron job that runs every 10 minutes in the Docker container. By
          default, containers shutdown once the main process running in the
          foreground ends. So we needed the container to stay up between runs.
          We implemented a solution to keep the container active that we’ll go
          into detail about in the ‘challenges’ section below.
        </p>
        <p>
          Finally, our Docker container is configured to send logs to GCP’s
          Stackdriver for easier debugging and monitoring. This proved to be a
          little tricky, and is also explored in the ‘challenges’ section.
        </p>

        <h3>
          4.3.2 Google Cloud Storage
        </h3>
        <p>
          The Embulk job running on the GCE instance formats that data into a
          newline-delimited-JSON file, which works well for loading data into
          BigQuery. However, instead of loading the data directly into BigQuery
          we have an intermediary step where the data is loaded to a staging
          bucket in GCS. We use the GCS buckets to provide a way to debug load
          issues if they occur, and archive data files in case they’re needed
          later. We’ll explain the archiving process in more detail shortly.
        </p>
        <p>
        [ INSERT IMAGE HERE ]
        </p>

        <h3>
          4.3.4 Cloud Function and BigQuery
        </h3>
        <p>
          When Embulk loads the data file into GCS, the Mashr Cloud Function is
          triggered. When the function triggers it first checks if the
          destination table named in the configuration file exists in the
          BigQuery dataset. If not, Mashr will create the table before
          attempting to load data.
        </p>
        <p>
          The Mashr Cloud Function’s primary purpose is to load the data file
          from the GCS staging bucket into BigQuery. After a successful load,
          the Function moves the data file from the GCS Staging bucket into the
          Archive bucket. The Archive bucket uses Coldline storage so that
          large amounts of data can be stored cheaply. We’ll explain the
          purpose of the Archive bucket in the next section.
        </p>
        <p>
        [ INSERT IMAGE HERE ]
        </p>
        <p>
          Earlier, during the deployment process on the user’s local machine,
          the code for this Cloud Function is generated and stored in the
          user’s current working directory. This enables users to make
          customizations to it if desired. For example, they could implement
          customized data transformations or validations. 
        </p>
        <p>
          Throughout the process, the logs from the Cloud Function are also
          sent to Stackdriver so the user can easily debug if necessary.
        </p>

        <h3>
          4.3.5 Failover for BigQuery Loads
        </h3>
        <p>
          This brings us to how Mashr handles a failing load job and what the
          Archive bucket is for. As previously stated, when the load job is
          successful the Cloud Function moves the data file from the Staging to
          the Archiving bucket for long term storage. However, if the data
          fails to load to BigQuery, the Cloud Function leaves the file in the
          Staging bucket. This way a user can easily find data files that were
          not loaded into BigQuery and investigate the cause. For example, if a
          new field was added to the data, BigQuery will reject the load. Once
          the user identifies the issue they can fix the problem and then load
          the data themselves.
        </p>
        <p>
        [ INSERT IMAGE HERE ]
        </p>

        <h2>
          4.4 Mashr Commands
        </h2>
        <p>
          In order to understand how much work Mashr is doing for the
          developer, let's explore what each command is doing behind the
          scenes, starting with <code>init</code>.
        </p>

        <h3>
          4.4.1 init
        </h3>

        <p>
          When you run <code>init</code> a template YAML configuration file is
          created in your working directory called
          <code>mashr_config.yml</code>. You can see that it instructs the user
          to fill out the template before running <code>mashr deploy</code>:
        </p>
        <p>
        [ INSERT IMAGE HERE ]
        </p>
        <p>
          In this example you can see that we’ve run <code>init</code> and
          Mashr created a <code>mashr_config.yml</code> template file in the
          user’s current working directory.
        </p>
        <p>
        [ INSERT IMAGE HERE ]
        </p>
        <p>
          If you pass the <code>--template</code> flag to <code>init</code>,
          you can specify some different templates for various input data
          sources that Mashr has available, including <code>psql</code> and
          <code>http</code>. This example is of an <code>http</code> template.
          In the actual templates that are generated there are helpful comments
          in each section that explain what to input and how Mashr will use it.
        </p>
        <p>
          The user then only has to fill in the details and run <code>mashr
           deploy</code>, and Mashr will do the heavy lifting of setting up
          your data pipeline.
        </p>
        <p>
          This is the example <code>mashr_config.yml</code> file after the user
          has filled it out. The first section at the top, <code>mashr</code>,
          is where the user specifies their GCP credentials along with the
          table and dataset to send data to. Additionally, the user specifies
          an <code>integration_name</code>.  The <code>integration_name</code>
          is what Mashr uses to name the GCS buckets, the GCE Instance and the
          Cloud Function. This way, the user can find them easily in GCP’s
          console if needed.
        </p>
        <p>
        [ INSERT IMAGE HERE ]
        </p>
        <p>
          Also in the <code>mashr</code> section, the user can specify what
          command to run Embulk with, and any dependencies or gems that this
          integration needs to run. These will be used later when we set up the
          Docker container with Embulk running on it inside the GCE instance.
        </p>
        <p>
          Everything inside the <code>embulk</code> section specifies what the
          input should be. You can see that the user has specified an input
          type of <code>http</code> and the url to <code>get</code> from.
        </p>

        <h3>
          4.4.2 Deploy
        </h3>
        <p>
          After filling out the <code>mashr_config.yml</code> file, the user
          runs <code>mashr deploy</code> from the same directory, and Mashr
          will launch all the resources necessary in GCP to create the data
          pipeline.
        </p>
        <p>
        [ INSERT IMAGE HERE ]
        </p>
        <p>
          Deploy does a lot of work for you in the background. As a broad
          overview, deploy performs these steps:

        <ol>
          <li>
            Performs local validations to ensure that the
            <code>integration_name</code>, table name and dataset name fit
            GCP’s naming conventions.
          </li>
          <li>
            Performs remote validations with GCP to ensure that the names for
            the Cloud Function, GCE Instance and GCS Buckets are not in use. As
            an example, GCS bucket names need to be globally unique across all
            of GCP.
          </li>
          <li>
            Copies a cloud function template to the current working directory
            so that the user can edit it if they wanted to add validations or
            transformations inside their data pipeline.
          </li>
          <li>
            Generates various files needed for the Embulk container including
            the crontab, the installation script for gems that our Embulk
            instance will need, and the Embulk yaml file used to extract data
            from the source.
          </li>
          <li>
            Asynchronously launches all of the GCP resources we talked about
            earlier: the GCE Instance, the GCS Buckets, the Cloud Function, and
            the BigQuery dataset and table.
          </li>
          <li>
            Finally, once the GCE Instance is created, the docker image is
            built and the docker container is started using the files generated
            in step 4.
          </li>
        </ol>
        </p>
        <p>
          Here is a diagram showing some of those major steps.
        </p>
        <p>
        [ INSERT IMAGE HERE ]
        </p>

        <h3>
          4.4.3 destroy
        </h3>
        <p>
        [ INSERT IMAGE HERE ]
        </p>
        <p>
          Finally, if you want to take down all of those resources you can run
          <code>mashr</code>. We considered a scenario where a user may have
          already deleted a resources before running <code>mashr
          destroy</code>, and therefore if a GCP resource doesn’t exist Mashr
        will only throw a warning while attempting to destroy GCP resources
        that don’t exist.
        </p>

        <h2>
          4.5 Summary
        </h2>
        <p>
          To recap, the principles of a good data pipeline include the
          following:
          <ul>
            <li>
              Extraction
            </li>
            <li>
              Validation
            </li>
            <li>
              Transformation
            </li>
            <li>
              Staging / Archiving
            </li>
            <li>
              Publishing to target
            </li>
            <li>
              Monitoring
            </li>
            <li>
              Orchestration and Ordering
            </li>
            <li>
              Performance
            </li>
          </ul>
        </p>


        <p>
          Mashr makes it easy to deploy a data pipeline with best practices
          considered:
          <ul>
            <li>
              Mashr orchestrates the extraction of data through Embulk.
            </li>
            <li>
              Mashr provides for the staging and archiving of load jobs to enable auditing, protect against failover, and provide easy debugging.
            </li>
            <li>
              Mashr performs monitoring for each step of the process through Stackdriver.
            </li>
            <li>
              Mashr provides an easy way to include validations and transformations if a user needs them, with customizable cloud functions.
            </li>
            <li>
              Mashr eases deployment with a simple YAML file which manages the setup of all the necessary resources of a data pipeline.
            </li>
            <li>
              Mashr provides the user full control over their data and customizability for each step of the data pipeline process, should a developer need it.
            </li>
          </ul>
        </p>
